**Ошибка** — величина, отражающая расхождение между ожидаемым и полученным ответами, и она должна идти на спад с каждой итерацией. 
Если этого не происходит, значит алгоритм работает неверно. Ошибку можно вычислить разными путями; примеры методов подсчета ошибки: 
[Sum of Squared Errors](https://en.wikipedia.org/wiki/Residual_sum_of_squares#:~:text=In%20statistics%2C%20the%20residual%20sum,actual%20empirical%20values%20of%20data).&text=A%20small%20RSS%20indicates%20a,the%20model%20to%20the%20data.) 
(SSE), [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE), [Root MSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation). 

Так как при инициализации мы задаем случайный набор весов и результат работы НС едва ли будет лучше, чем случайный, нам 
необходимо изменить веса так, чтобы НС выдавала верный результат (то есть чтобы выходные данные классификации совпали с известными нам 
метками классов). В нашем случае верным результатом будет предсказание вида ириса по длине и ширине лепестков, которое совпадет со
значением, записанным для данного объекта в колонке "species". Необходимое изменение весов достигается с помощью обратного распространения ошибки (backward propagation).

<a href="https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F_%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B8#:~:text=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%20%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B3%D0%BE%20%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F%20%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B8%20(%D0%B0%D0%BD%D0%B3%D0%BB,%D0%B1%D1%8B%D0%BB%20%D0%BE%D0%BF%D0%B8%D1%81%D0%B0%D0%BD%20%D0%B2%201974%20%D0%B3.">Алгоритм обратного распространения ошибки</a> — 
популярный алгоритм обучения нейронных сетей прямого распространения. Он относится к методам обучения с учителем, и поэтому необходимо, 
чтобы в обучающих примерах были заданы целевые значения (например, у нас известно, какие ирисы имеют какие характеристики).

Обратное распространение ошибки работает с использованием функции потерь, показывающей, насколько далеко сеть была от верного результата.

<h2>Потери</h2>

Для вычисления функции потерь мы будем использовать суммарную квадратичную ошибку (SSE):

$$Loss(y, \hat{y}) = \sum\limits_{i=1}^{n} (y_i - \hat{y}_i) ^ 2$$

где $\hat{y}$ – предсказанные выходные данные;

$y$ – реальные выходные данные.

Сумма квадратов ошибок (Sum of Squared Errors, SSE) – сумма разностей между каждым спрогнозированным значением и реальными данными. 
Разность возведена в квадрат, чтобы оперировать ее абсолютным значением. Это значение – мера неточности работы нашаей нейросеть, и его нужно минимизировать.

Задача обучения – найти такой набор весов и смещений, который наилучшим образом минимизирует функцию потерь. Для того чтобы узнать, на сколько и в 
каком направлении нужно изменить веса и смещения, необходимо знать зависимость производной функции потерь от них.

<div class="hint">Стоит отметить, что задача нахождения глобального минимума этой функции, как правило, очень сложна, и мы, 
скорее всего, получим какой-то из локальных минимумов, который может быть лучше или хуже, но в каком именно из них мы окажемся – 
зависит от изначального случайного набора весов и смещений.</div>

В разделе Градиентный спуск (<a href="https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,the%20direction%20of%20steepest%20descent.">Gradient Descent</a>) 
предыдущего урока рассказывалось, что производная (или же <a href="https://ru.wikipedia.org/wiki/%D0%93%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82">градиент</a>) показывает наклон 
графика функции. Если известна производная функции потерь, то мы знаем, в каком направлении уменьшается ее значение, и можем обновлять веса 
и смещения, увеличивая или уменьшая их в зависимости от этого значения.

Тем не менее, невозможно просто вычислить зависимость производной функции потерь от весов и смещений, так как уравнение функции не содержит в себе 
ни весов, ни смещений. Для подобных вычислений необходимо определить некое связующее правило.
$$\frac {\partial Loss(y, \hat{y})}{\partial W} = \frac { \partial Loss(y, \hat{y} ) } {\partial \hat{y}} \frac { \partial \hat{y} } {\partial z} \frac { \partial z } {\partial W} $$
$$= 2 (y - \hat{y} ) * z (1- z) * x$$
где $z = Wx + b$

Вот так можно вычислить прирост весов, то есть значения, которые будут добавлены к весам нейронов выходного ($\delta_{o}$) и скрытого слоя ($\delta_{h}$):


$$\delta_o=(OUT_{real} - OUT_{actual}) * f_a'(OUT_{real})$$   
$$\delta_h=f_a'(OUT_h) * (w_i * \delta_i)$$

Обновление же весов происходит по следующей формуле:

$$weight = weight + learning\\_rate * error * input$$

где $weight$ это вес; $learning\\_rate$ – “скорость обучения” – <a href="https://en.wikipedia.org/wiki/Learning_rate">параметр</a> настройки сети, который необходимо указать; $error$ – ошибка, 
вычисленная для нейрона на предыдущем шаге; $input$ – значение входных данных, на которых была получена эта ошибка.  

<div class="hint">В машинном обучении и статистике скорость обучения – это параметр настройки в алгоритме оптимизации, 
который определяет размер шага на каждой итерации при приближении к минимуму функции потерь.</div>


<h2>Задание</h2>
В файле `network.py` реализуйте метод `backward` класса `NN`, который производит следующие операции:

<ul>
<li>Посчитать ошибку для выходного слоя (<code>delta_l2</code>) как разницу между результатами работы сети (<code>output</code>) и реальными метками классов (<code>y</code>) и умножить поэлементно на производную функции активации по output (формула для $\delta_{o}$).</li>
<li>Посчитать ошибку для скрытого слоя (<code>delta_l1</code>) как произведение матриц ошибки выходного слоя и весов <code>w2</code>, умноженное поэлементно на производную функции активации по выходным данным скрытого слоя (<code>layer1</code>) (формула $\delta_{h}$).</li>
<li>Скорректировать весовые коэффициенты выходного слоя (<code>w2</code>), вычислив векторное произведение скрытого слоя (<code>layer1</code>) и ошибки выходного слоя (<code>delta_l2</code>), умноженное поэлементно на learning rate (формула 3).</li>
<li>Скорректировать весовые коэффициенты скрытого слоя (<code>w1</code>), вычислив векторное произведение входного слоя (<code>X</code>) и ошибки скрытого слоя (<code>delta_l1</code>), умноженное поэлементно на learning rate (формула 3).</li>
</ul>

Прежде чем приступать, удалите оператор `pass` и раскомментируйте все строки, не являющиеся пояснениями к заданию.
Производная функции активации реализована в модуле `derivative.py`.

<div class="hint">При умножении матриц какие-то из них будет нужно транспонировать!</div>


Для того чтобы посмотреть на результаты работы кода, вы можете добавить следующие строки в блок `if __name__ == '__main__':` в `task.py` и запустить его:

```python
print(f'w1 before backward propagation: \n{nn.w1} \nw2 before backward propagation:\n{nn.w2}')
nn.backward(X_train, y_train, output)
print(f'w1 after backward propagation: \n{nn.w1} \nw2 after backward propagation:\n{nn.w2}')
```
Этот код позволит вам увидеть, как изменятся веса после обратного распространения ошибки.