<html>
<h4><i>О чем полезно иметь представление для успешного прохождения этого урока:</h4>

<ul>
  <li>Линейная алгебра: в частности, операции с матрицами</li>
  <li>Дифференциальное исчисление</li>
  <li>Векторный анализ</li></i>
</ul>

<p>В этом уроке мы мы будем строить классификационную модель на основе данных о длине и ширине лепестков разных
  видов ириса с использованием нейронной сети.</p>

<h2>Нейросети и нейроны</h2>

<p><a href="https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C">Нейронная сеть</a> (или же искусственная нейронная сеть, ИНС) — это математическая модель, построенная по принципу
организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма. Это понятие 
  впервые возникло в середине ХХ века при изучении процессов, протекающих в мозге, и при попытке смоделировать эти процессы.</p>

<p>Мы будем рассматривать пример самого базового типа нейронных сетей — это <a href="https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C_%D1%81_%D0%BF%D1%80%D1%8F%D0%BC%D0%BE%D0%B9_%D1%81%D0%B2%D1%8F%D0%B7%D1%8C%D1%8E">сеть прямого распространения</a> (СПР,
англ. feedforward neural network), где все связи направлены строго в одном направлении &mdash; от входа
к выходу.</p>

<p>Нейронные сети используются для решения сложных задач, которые требуют аналитических вычислений. Наиболее
распространенными применениями нейронных сетей являются:</p>


<ul>
  <li>Классификация</li>
  <li>Предсказание</li>
  <li>Распознавание</li></i>
</ul>


<h3>Нейрон</h3>

<p>Структурной единицей нейронных сетей является <a href="https://ru.wikipedia.org/wiki/%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD">нейрон</a>. Это некая функция, получающая входные данные,
производящая над ними математические преобразования и выдающая числовой результат.
Нейроны делятся на три основных типа: входной, скрытый и выходной. Если нейросеть состоит из 
большого числа нейронов, вводят термин слоя. Соответственно, есть входной слой (он принимает информацию), 
скрытые слои (обычно не больше 3), которые обрабатывают информацию, и выходной слой, который выводит результат.  
У каждого из нейронов есть входные (input data) и выходные данные (output data). В случае входного нейрона 
input=output. У всех остальных в input попадает суммарная информация со всех нейронов предыдущего слоя, а 
результат вычислений попадает в output.</p>

<figure>
  <img src="neuron-scheme.png" alt="Neuron" style="width:100%">
  <figcaption>Схема искусственного нейрона.</figcaption>
</figure>

<p>Сначала нейрон складывает значения всех входных данных. На изображении представлены $n$ входных сигналов ($x^1, x^2, \dots x^n$ ).</p>

<p>Эти значения перед сложением умножаются на весовые коэффициенты (weight) ($w_1, w_2, \dots w_n$), то есть вычисляется
взвешенная сумма. Веса могут быть как положительными, так и отрицательными, и они будут изменены в процессе обучения. 
Для корректировки к полученному выходному значению может быть добавлено смещение $b$ (bias).</p>

<div class="hint">Это может быть нужно, если мы хотим, чтобы данный нейрон имел bias быть “активным” при более низком 
или более высоком значении суммы: например, чтобы нейрон "активировался", когда взвешенная сумма не просто больше 0, 
а больше 10, мы добавляем смещение -10.</div>

<p>Так как нам нужно будет работать с разнородными входными данными, масштаб которых может сильно различаться,
необходимо перенести значения на простой, известный, жесткий диапазон. Этот процесс называется **нормализацией**, и он 
часто используется в нейронных сетях. После сложения входных данных и добавления смещения нейрон применяет к полученному 
значению <a href="https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%B0%D1%86%D0%B8%D0%B8">функцию активации</a> (activation function) $\sigma$. Функция активации “сжимает” (нормирует) числовой ряд в промежуток
от 0 до 1 или от -1 до 1. Использование логистической сигмоиды дает значения в диапазоне [0,1], а использование, например, 
tanh – в диапазоне [-1,1].</p>

<p>В классическом варианте в качестве функции активации используется логистическая <a href="https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D0%B3%D0%BC%D0%BE%D0%B8%D0%B4%D0%B0">сигмоида</a> (sigmoid function),
она стала популярной по историческим причинам. В современных ИНС такая сигмоида используется редко, поскольку оказалось, 
что с этой функцией нейросети часто обучаются хуже, чем с некоторыми другими. Примеры функций активации:</p>

<ul>
  <li>Логистическая сигмоида:
    $$\sigma(x) = \frac1{1+e^{-x}}$$</li>
  <li>Гиперболический тангенс:
    $$tanh(x) = 2\sigma(2x) - 1$$</li>
  <li>Rectified Linear Unit (ReLU):
    $$f(x) = max(0,x)$$</li></i>
</ul>

<div class="hint">Логистическая сигмоида может приводить к “застреванию” нейросети во время обучения, поскольку 
при больших отрицательных входных значениях на выходе она выдает значения близкие к нулю. Из-за этой особенности 
обновление весов будет происходить довольно медленно. Напротив, выход функции tanh находится в диапазоне [-1, 1], и 
сильно отрицательные значения на входе будут давать отрицательные значения на выходе; только околонулевые значения на 
входе будут давать околонулевые значения на выходе. Это делает сеть менее склонной к “застреванию” во время обучения. 
Сигмоиды также могут вызывать проблему исчезающего градиента. Выпрямители, такие как ReLU, меньше страдают от <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">проблемы
исчезающего градиента</a>, более просты с точки зрения вычислений и на практике позволяют нейросетям сходиться быстрее, чем
сигмоидные функции (<a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">источник</a>). </div>

<p>Формула для выходных данных нейрона:</p>
$$a = \sigma(\sum\limits_{j=1}^n w_j x^j + b)$$

<p>Чуть более наглядно эту формулу можно представить вот так:</p>

<p>
$$a = \sigma\left(
\begin{bmatrix}
w_{0,0} &
w_{0,1} &
\cdots &
w_{0,n} \\
w_{1,0} &
w_{1,1} &
\cdots &
w_{1,n} \\
\vdots &
\vdots &
\ddots &
\vdots \\
w_{n,0} &
w_{n,1} &
\cdots &
w_{n,n}
\end{bmatrix}
\begin{bmatrix}
a_{0}^{(0)} \\
a_{1}^{(0)} \\
\vdots \\
a_{n}^{(0)}
\end{bmatrix}
+
\begin{bmatrix}
b_{0} \\
b_{1} \\
\vdots \\
b_{n}
\end{bmatrix}
\right)$$
</p>

<h2>Задание</h2>
В файле <code>activation.py</code> реализуйте логистическую функцию активации <code>sigmoid()</code>, пользуясь формулой, приведенной выше.

</html>

> <i>Этот курс сейчас в альфа-версии. Пожалуйста, помогите нам его улучшить. Для этого вы можете ответить
> на вопросы к каждому из заданий данного урока в опроснике по <a href="https://docs.google.com/forms/d/e/1FAIpQLSc7uQK8wSbhRUnReXtUeMKE0eRy6JjlutTH7iEbKzwKL1VV5g/viewform?usp=sf_link">ссылке</a>.
> Cпасибо :) </i>