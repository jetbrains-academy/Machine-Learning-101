Теперь нам необходимо преобразовать формулу вычисляемой вероятности так, чтобы ее можно 
было рассчитать, используя частоту встречаемости слов. Для этого можно использовать 
некоторые базовые свойства вероятностей. Вспомним формулу расчета вероятности события 
$A$ при условии наступлении события $B$:

$$P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}$$

В нашем случае нужно рассчитать вероятность того, что при данном наборе слов в сообщении, это сообщение&nbsp;— спам, 
то есть $P(Spam|sentence)$. Так, с помощью теоремы Байеса мы получаем:

$$P(Spam|sentence) = \frac{P(sentence|Spam) \times P(Spam)}{P(sentence)}$$

В нашем классификаторе мы лишь пытаемся определить наиболее вероятный класс, так что мы 
можем пренебречь знаменателем, который будет одинаковым для обоих классов, и сравнить 
только числители:

$$P(sentence|spam) \times P(spam)$$

и

$$P(sentence|ham) \times P(ham)$$

**Наивный байесовский классификатор**&nbsp;— это алгоритм для задач двухклассовой и мультиклассовой
классификации. Он называется наивным, потому что вычисления вероятностей для каждого класса 
упрощены, чтобы сделать их вычисление более удобным. Предполагается, что наличие какого-либо 
из признаков не связано с наличием остальных. Это очень сильное допущение, которое едва ли 
будет выполняться в реальных данных, т.е. маловероятно, что признаки не взаимодействуют. Тем не 
менее, этот подход зачастую удивительно хорошо работает даже на данных, где это предположение не 
выполняется.

В нашем случае описанное выше допущение означает, что вероятность встретить некоторое слово 
в сообщении не зависит от наличия других слов в этом сообщении.

$$P(\text{Who let the dogs out}) = P(\text{Who}) \times P(\text{let}) \times P(\text{the}) \times P(\text{dogs}) \times P(\text{out})$$


### Задание

В файле `bayes.py` pеализуйте метод `fit` класса `NaiveBayes`, который по переданной выборке 
вычисляет и сохраняет в виде атрибутов класса следующие параметры, которые понадобятся на этапе 
классификации:
- `classes_prior`&nbsp;— оценка априорной вероятности классов в виде numpy вектора длины 2 
(количество классов). Вычисляется как доля каждого класса во всей выборке:
  $$P (\text{spam}) = \frac{N_{spam}}{N_{documents}}$$

- `classes_words_count`&nbsp;— суммарное количество слов для сообщений каждого класса в 
  виде вектора длины 2. Для расчета используйте матрицу X (полученную через `vectorize(X)`) 
  и маску `y_i_mask`. Пригодится функция [numpy.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html).
- `likelihood`&nbsp;— относительные частоты слов для каждого класса в виде вектора numpy 
  размерности (2, M), где M&nbsp;— размер словаря. Для расчета частоты встречаемости каждого 
  слова в каждом из классов сначала нужно рассчитать, сколько всего раз данное слово встретилось 
  во всех сообщениях этого класса, используя матрицу X (полученную через `vectorize(X)`) и 
  маску `y_i_mask`, а затем разделить каждый элемент на общее число слов в сообщениях класса 
  (`classes_words_count`).
  
<div class="hint">
Функция numpy.sum позволяет суммировать как абсолютно все элементы массива, так и только лишь 
элементы вдоль выбранной оси. По умолчанию <code>axis=None</code>, то есть суммируются все элементы массива.</div>

Чтобы посмотреть, как работает ваш код, вы можете запускать `task.py`.
В этом задании добавьте следующие строки в блок `if __name__ == '__main__':`, прежде чем
запускать код:
```python
nb = NaiveBayes()
nb.fit(X_train, y_train)
print('Total number of words in each class: ', nb.classes_words_count)
print('Class prior probabilities: ', nb.classes_prior)
print('Relative word frequencies for each class: ', nb.likelihood)
```
Кроме того, импортируйте модуль с необходимым классом в `task.py`:
```python
from bayes import NaiveBayes
```
