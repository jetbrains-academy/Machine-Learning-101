В данном уроке мы создадим наивный [байесовский классификатор](http://www.machinelearning.ru/wiki/index.php?title=%D0%9D%D0%B0%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9_%D0%B1%D0%B0%D0%B9%D0%B5%D1%81%D0%BE%D0%B2%D1%81%D0%BA%D0%B8%D0%B9_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80) (Naive Bayes Classifier) для определения 
спама в СМС сообщениях. Несмотря на большие успехи машинного обучения в последние годы, наивный 
байесовский алгоритм остается не только одним из простейших, но и одним из самых быстрых, точных 
и надежных. Он успешно используется для многих целей (рекомендательные системы,
классификация данных в режиме реального времени и т.д.), но особенно хорошо работает с задачами 
обработки естественного языка.

<style>
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
</style>
![bayes](Thomas_Bayes.png)

Наивный байесовский классификатор основан на [теореме Байеса](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%B0). Теорема Байеса&nbsp;— это 
математическая формула, используемая для вычисления условных вероятностей.

**Условная (апостериорная) вероятность**&nbsp;— это вероятность наступления одного события при у
словии, что другое событие (по предположению, допущению, подтвержденному или неподтвержденному 
доказательством утверждению) уже произошло.

Формула для определения условной вероятности:

$$P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}$$

Она показывает, как часто происходит событие $A$ при наступлении события $B$. При этом мы должны знать:
- Как часто происходит событие $B$ при наступлении события $A$, что обозначается в формуле как $P(B|A)$.
- Безусловную (априорную) вероятность события A, обозначаемую в формуле как $P(A)$.
- Безусловную вероятность события B. В формуле она обозначается как $P(B)$.

Можно сказать, что теорема Байеса&nbsp;— это способ определения вероятности, исходя из знания других вероятностей.

<div class="hint"><b>Парадокс Теоремы Байеса</b>. Пусть существует заболевание с частотой распространения 
среди населения 0,001 и метод диагностического обследования (тест), который с вероятностью 0,9 
выявляет больного, но при этом имеет вероятность 0,01 ложноположительного результата&nbsp;— ошибочного 
выявления заболевания у здорового человека. Нужно найти вероятность того, что человек на самом деле 
здоров, если тест показал, что он болен. По теореме Байеса получается, что эта вероятность 91.7%, то есть 
большинство людей, у которых тест показал результат «болен», на самом деле здоровы. Причина этого в том, 
что по условию задачи вероятность ложноположительного результата хоть и мала, но на порядок больше доли 
больных в обследуемой группе людей. Расчеты и более подробное объяснение можно найти <a href="https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%B0#%D0%9F%D1%80%D0%B8%D0%BC%D0%B5%D1%80_4_%E2%80%94_%D0%BF%D0%B0%D1%80%D0%B0%D0%B4%D0%BE%D0%BA%D1%81_%D1%82%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D1%8B_%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%B0">здесь</a>. </div>

**Наивный классификатор**&nbsp;— вероятностная модель. Он вычисляет вероятность каждого из классов для 
данного текста и возвращает наиболее вероятный.

Нашей задачей будет вычислить вероятности того, что конкретное предложение это "Spam" или "Ham", а затем 
выбрать наиболее вероятный вариант.

$P(Spam|sentence)$&nbsp;— вероятность того, что предложение это “Spam” при условии конкретного предложения, 
то есть учитывая набор слов в этом предложении.


### Задание

В файле "Spam.txt" находится датасет, содержащий размеченные сообщения. Первое слово каждой 
строки&nbsp;— это идентификатор класса “Spam” или “Ham”, далее через табуляцию следует само сообщение.

Прежде чем строить классификатор, нужно привести данные в удобный для классификации формат. Для этого 
воспользуемся стандартной моделью для текстов под названием [Bag of words](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%88%D0%BE%D0%BA_%D1%81%D0%BB%D0%BE%D0%B2). Она позволяет вычислить 
количество вхождений каждого слова в документе, независимо от порядка слов и построения предложений.

В файле `vectorize.py` уже реализована функция `split_by_words()`, которая принимает на вход вектор строк 
и возвращает вектор из списков отдельных слов, которые были в каждой строке. Она использует следующие функции:

- [numpy.char.lower](https://numpy.org/doc/stable/reference/generated/numpy.char.lower.html) — возвращает массив элементов, преобразованных в нижний регистр.
- [numpy.char.translate](https://numpy.org/doc/stable/reference/generated/numpy.char.translate.html#numpy-char-translate) — позволяет преобразовать строку, применяя к каждому символу заданное преобразование.
- [numpy.char.split](https://numpy.org/devdocs/reference/generated/numpy.char.split.html#numpy-char-split) — для каждого элемента в массиве (строки) возвращает список слов.
- [str.maketrans](https://docs.python.org/3/library/stdtypes.html#str.maketrans) — возвращает таблицу перевода символов (translation table). Использует три 
  аргумента&nbsp;— x, y, z&nbsp;— где ‘x’ и ‘y’&nbsp;— строки одинаковой длины и символы в ‘x’ заменяются 
  на символы ‘y’. Аргумент ‘z’&nbsp;—  строка (в нашем случае, string.punctuation), все символы из которой 
  заменяются на `None`. Это позволяет избавиться от знаков препинания.

В этом же файле реализуйте функцию `vectorize`. Она должна:
1) Найти число сообщений на входе.
2) Получить из них вектор списков отдельных слов, используя функцию `split_by_words()`.
3) Получить из него одномерный массив уникальных слов.
4) Создать словарь, в котором каждому уникальному слову будет соответствовать индекс.
5) Cоздать матрицу, размера (N, M), где M&nbsp;— размер словаря. В каждой строке матрицы на j-й позиции 
   находится число x, которое означает, что j-е слово встретилось в i-м сообщении x раз.
6) Вернуть словарь и матрицу.

<div class="hint">
В данном задании могут быть полезны следующие функции:
<a href="https://numpy.org/doc/stable/reference/generated/numpy.unique.html?highlight=unique#numpy.unique">numpy.unique</a>&nbsp;— c этой функцией мы уже неоднократно сталкивались в предыдущих уроках.
<a href="https://numpy.org/doc/stable/reference/generated/numpy.hstack.html">numpy.hstack</a>&nbsp;— производит конкатенацию массивов по второй оси, за исключением одномерных массивов, 
где конкатенация происходит по первой оси.
</div>

Чтобы посмотреть, как работает ваш код, вы можете запускать `task.py`. В этом задании модифицировать этот файл не нужно.

> <i>Этот курс сейчас в альфа-версии. Пожалуйста, помогите нам его улучшить. Для этого вы можете ответить
> на вопросы к каждому из заданий данного урока в опроснике по <a href="https://docs.google.com/forms/d/e/1FAIpQLSd3V5XUAMjCyU4uOuri9WKBEXpVsRfzCfMfVtnS8AzjqdXqFw/viewform?usp=sf_link">ссылке</a>.
> Cпасибо :) </i>