Одна из проблем с наивным подходом заключается в следующем: если какое-то слово не встречалось 
в тренировочной выборке класса Spam, то его вероятность:
$$P(word|Spam)=0$$

Это называется проблемой нулевой частоты. Она приведет к тому, что сообщение с этим словом 
нельзя будет классифицировать, так как оно будет иметь нулевую вероятность во всех классах. 
Ликвидировать эту проблему путем анализа большего количества документов не получится, так 
как составить обучающую выборку, содержащую все возможные слова, включая синонимы, неологизмы, 
опечатки и т.д, невозможно.

Одним из часто используемых решений является [аддитивное сглаживание](https://en.wikipedia.org/wiki/Laplace_smoothing), или же сглаживание 
Лапласа&nbsp;— техника, позволяющая сгладить категориальные данные (т.е. качественно 
характеризующие исследуемый процесс или объект, не имеющие количественного выражения). При 
наблюдаемом ${\textstyle \textstyle {\mathbf {x} \ =\ \left\langle x_{1},\,x_{2},\,\ldots ,\,x_{d}\right\rangle }}$ 
из мультиномиального распределения с ${\textstyle \textstyle {N}}$ 
испытаний, “сглаженный” вариант данных дает нам оценку:


$$\hat{\theta_i}={\frac{x_i + \alpha}{N+\alpha d}}\qquad (i=1, \ldots , d),$$


где $α > 0$ — параметр сглаживания. $α = 0$ соответствует отсутствию сглаживания.

По сути, при $α = 1$ мы делаем вид, будто встречали каждое слово на один раз больше, то есть 
прибавляем единицу к частоте каждого слова. Таким образом, слова, которые не попались на этапе 
обучения модели, получают маленькую, но все же не нулевую вероятность. В противовес этому мы 
добавим количество возможных слов к знаменателю, чтобы результат деления никогда не превышал единицы:

$$P(w_i|c)={\frac{W_{ic}+1}{\sum_{i'\in|V|}{(W_{i'c}+1)}}}={\frac{W_{ic}+1}{|V|+\sum_{i'\in|V|}{W_{i'c}}}}$$

где: 

$P(w_i|c)$ — вероятность слова в классе;

$W_{ic}$ — сколько раз $i$-ое слово встречается в сообщениях класса $c$;

$V$ — список всех уникальных слов.


### Задание
Обновите свою имплементацию метода `fit` так, чтобы использовать сглаживание Лапласа.
В коде уже реализован параметр `alpha`.

<div class="hint">
Вам нужно поменять начальную имплементацию атрибута <code>likelihood</code>, а также расчет 
значения знаменателя. </div>

<div class="hint">
Массив <code>likelihood</code> изначально должен быть заполнен не нулями, а $\alpha=1$. </div>

Чтобы посмотреть, как работает ваш код, вы можете запускать `task.py`.
В этом задании модифицировать `task.py` не нужно. Обратите внимание, как изменились значения 
вероятностей по сравнению с предыдущим шагом.
