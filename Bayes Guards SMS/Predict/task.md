В нашем случае переменная класса имеет только два возможных значения: 
Spam или Ham. Конечно же бывают случаи, когда классификация является многомерной. 
Поэтому необходимо найти переменную класса с максимальной вероятностью. Используя 
приведенную ниже формулу алгоритма классификации, можно получить такой класс, исходя из 
имеющихся предикторов.

$$y=\arg\max\limits_{y \in Y}  \prod  P(y) \times  P(x_j |y)$$

$y$ — классы Spam или Ham;

$x_j$ — j-е слово в предложении.


При достаточно большой длине сообщения придётся перемножать большое количество очень
маленьких чисел, что может привести к ситуации, когда результат операции с
плавающей запятой становится настолько близким к нулю, что порядок числа выходит за
пределы разрядной сетки и компьютер не может представить его в памяти. Такая ситуация
называется [исчезновением порядка](https://ru.wikipedia.org/wiki/%D0%98%D1%81%D1%87%D0%B5%D0%B7%D0%BD%D0%BE%D0%B2%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BA%D0%B0), а стандартный способ избежать его&nbsp;— применить
логарифм к выражению, стоящему под $argmax$. Таким образом, формула для нашего алгоритма
преобразуется в такой вид:

$$ \arg\max\limits_{y \in Y} [ \log(P(y)) + \sum\limits_{j=1}^{|V|} log(p(x_j |y))]$$

## Задание
В файле `bayes.py`  реализуйте метод `predict` класса `NaiveBayes`, принимающий 
массив объектов `X` и возвращающий список соответствующих меток классов. Прежде 
чем приступать, удалите оператор `pass` и раскомментируйте все строчки, которые 
не являются пояснениями. 

<div class="hint">Вы можете выделить сразу весь блок с комментариями и нажать Ctrl + / 
(Windows, Linux) или ⌘ + / (MacOs), чтобы раскомментировать нужные строки. </div>

- Сначала каждое сообщение внутри массива превратить в вектор отдельных слов при 
  помощи функции `split_by_words()`.
- В каждом сообщении найти набор уникальных слов и создать вектор нулей такой же 
  размерности.
- Для каждого уникального слова из полученного списка найти соответствие в словаре, 
  если оно находится&nbsp;— записать его индекс в вектор, созданный на предыдущем 
  шаге, если нет&nbsp;— записать индекс, равный длине словаря. Все такие слова имеют 
  одинаковую вероятность.
- Рассчитать `log_likelihood`, применив функцию `np.log()` к срезу массива `likelihood`, 
  полученного при помощи `index_array`&nbsp;— таким образом мы оставляем в массиве 
  только вероятности тех слов, которые есть в нашем предложении. 
- Используя приведенную выше формулу, рассчитать наиболее вероятный класс для этого 
  сообщения.
- Вернуть список наиболее вероятных классов для всех сообщений из входного массива.


Также реализуйте метод `score`, который прогоняет тестовую выборку через алгоритм, а затем сравнивает 
полученные метки классов с реальными и возвращает долю верно классифицированных объектов.

<div class="hint">
Апостериорные вероятности для каждого класса рассчитываются как сумма логарифма 
априорной вероятности и суммы логарифмов вероятностей для слов из <code>log_likelihood</code>, 
сложенных вдоль 1й оси, то есть отдельно для каждого класса.
</div>

<div class="hint">
После нахождения апостериорных вероятностей для классов, нужно определить, какая 
из них больше, и выбрать из <code>unique_classes</code> тот, который ей соответствует. Здесь 
может пригодиться функция <a href="https://numpy.org/doc/stable/reference/generated/numpy.argmax.html">numpy.argmax</a>.
</div>

Для того чтобы посмотреть на результаты работы своего кода, вы можете добавить следующие
строчки в блок `if __name__ == '__main__':` в `task.py`, после чего запустить его:

```python
print(nb.predict(["This is not a spam"]))
print("Score:")
print(nb.score(X_test, y_test))
print(nb.score(X_train, y_train))
```
