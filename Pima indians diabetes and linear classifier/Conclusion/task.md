In this lesson, we've discussed an example of implementing the linear classifier utilizing the stochastic gradient descent.

Among all local optimization methods, this method is the easiest in implementation. Nevertheless, it is a powerful tool for tasks of this kind. We've considered a classification example with two classes – the presence or absence of type 2 diabetes. However, the algorithm may be used with more classes, too, even if then it will require considerable modifications.

In real-life tasks, you don't need to manually reproduce the algorithm from scratch – it has been implemented, for example, in the [scikit](https://scikit-learn.org/stable/modules/sgd.html) library. When using that version, you will need to pass the parameters we've discussed here to the function: the number of iterations for the "optimistic strategy" (`n_iter`) or the kind of the `loss` function. Among other things, the algorithm implemented in *scikit* has built-in settings to avoid exploding gradients.

The stochastic gradient descent method may be used for setting up the [Support Vector Machine method](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2) or even other linear classifiers and regressors. It is often used in text classification or in Natural Language Processing, as well as in setting up a single-layer perceptron or [neural networks](http://www.machinelearning.ru/wiki/index.php?title=%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C). We will discuss it later in the course.