### K-means

На этом уроке мы рассмотрим задачу кластеризации — разбиения выборки на непересекающиеся множества, объекты которых схожи между собой и отличаются от объектов других множеств. В нашей задаче объектами будут точки изображения, а их характеристиками — значения трех цветовых компонент. Центры множеств будут определяться теми цветами, на которые мы заменили исходные цвета. Например, несколько различных оттенков красного — от яркого до бордового — могут быть объединены в один, усредненный красный, который и станет центром кластера.


Алгоритм кластеризации при помощи k-средних (k-means algorithm) относится к категории алгоритмов обучения без учителя, применяемых тогда, когда мы заранее не знаем, на какие именно группы (кластеры) мы разнесем объекты. Мы не можем заранее сказать, в какие цвета будет перекрашено наше изображение, задача алгоритма — вычислить эти цвета за нас.
В данном случае алгоритм должен разбить выборку на `k = 8` групп.

Суть алгоритма — в итеративном присвоении каждой из точек выборки одной из `k` групп на основе имеющихся признаков (в нашем случае это исходные цвета). Кластеры образуются из точек со схожими признаками.

В алгоритме применяется итеративное улучшение результата. На вход подается количество кластеров и набор данных, содержащий признаки каждой из точек (в данном случае для каждой точки указаны значения красного, синего и зеленого каналов). Кластеры характеризуются своими центрами — центроидами. В самом начале алгоритм задает `k` центроидов случайным образом либо выбором наугад из входного набора данных. [Центроид](https://ru.wikipedia.org/wiki/%D0%91%D0%B0%D1%80%D0%B8%D1%86%D0%B5%D0%BD%D1%82%D1%80) — это предполагаемый геометрический центр кластера. Здесь мы говорим о координатах в пространстве признака, так центроид для черного (0, 0, 0) и белого (255, 255, 255) цветов будет в точке (127, 127, 127) – то есть серым! На начальном этапе вовсе не обязательно, чтобы он был близок к действительному центру (хоть это и может улучшить работу алгоритма). С каждой итерацией мы будем приближать центроиды к их действительным значениям.

Далее мы будем итеративно повторять два шага:

1. Шаг распределения точек: присваиваем каждой точке кластер с ближайшим центроидом. Более формально:

$$c_i = \underset{{c \in 1\dots k}}{\arg\min}  \rho(x_i, \mu_c)$$

где
- $c_i$ — центр кластера, присвоенный точке $x_i$;
- $\rho(x_i, \mu_c)$ — расстояние между точкой $x_i$ и центром кластера $\mu_c$;
- $\mu_{c}$ — центр кластера.

2. Шаг обновления центроидов: заново рассчитываем центроиды. Новым центроидом для каждого кластера становится среднее значение всех входящих в него точек.

$$ {\mu_{c} = \frac{\sum\limits_{j=1,\dots, n} [c_i = c] x_i^j}{\sum\limits_{c_i = c} 1} } $$


Алгоритм повторяет шаги 1 и 2 до тех пор, пока не достигается одно из условий останова: либо в результате последней итерации ни одна из точек не изменила свой кластер, либо была достигнута минимальная сумма расстояний, либо число итераций достигло установленного лимита. Условие останова здесь — момент, в который мы считаем работу алгоритма выполненной; при отсутствии этого условия он будет повторять итерации бесконечно.
При наличии данных условий алгоритм гарантированно сходится. Чем более строгие условия заданы, тем дольше алгоритм будет производить вычисления. Условия стоит варьировать: если производительность недостаточна, стоит ограничить количество итераций. Иногда имеет смысл пересмотреть исходное условие задачи (например, вместо 8 кластеров, мы можем попробовать построить 4).

Стоит учесть, что в случае, если мы изначально выбрали крайне неудачные центроиды или слишком большое число `k`, то на шаге 1 мы можем получить кластер, к которому не сможем отнести ни одну точку. Такие случаи необходимо отдельно обрабатывать на шаге 2, чтобы избежать ошибок при вычислении среднего. Здесь возможны различные подходы. К примеру, мы можем присвоить пустому кластеру в качестве центроида случайную точку либо точку, наиболее удаленную от центра наибольшего из имеющихся кластеров.

Для вычисления удаленности точек здесь мы используем [евклидову метрику](https://ru.wikipedia.org/wiki/%D0%95%D0%B2%D0%BA%D0%BB%D0%B8%D0%B4%D0%BE%D0%B2%D0%B0_%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%BA%D0%B0), однако, в зависимости от условий задачи, может применяться любая другая. К примеру, для решения задачи на кластеризацию текстов мы могли бы использовать [расстояние Левенштейна](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%9B%D0%B5%D0%B2%D0%B5%D0%BD%D1%88%D1%82%D0%B5%D0%B9%D0%BD%D0%B0). Функция для вычисления евклидовой метрики находится в файле `distances.py`.



### Задание

Реализуйте функцию, `k_means(X, n_clusters, distance_metric)`, которая принимает матрицу $X$ размерности `(n_samples, n_features)`, количество кластеров, на которые мы хотим разбить изображение, и метрику.

Результатом работы функции является пара из вектора размера `n_samples`, где в $i$-й ячейке содержится номер кластера, соответствующий $i$-му пикселю, и вектора размера `(n_clusters)` с центрами кластеров.

Заготовка для функции расположена в файле `clustering.py`. Там же находится функция `init_clusters`, создающая начальные центроиды для заданного набора данных. 

При выполнении задания вам может пригодиться функция [numpy.sum](https://numpy.org/doc/1.18/reference/generated/numpy.sum.html), вычисляющая сумму элементов массива.
