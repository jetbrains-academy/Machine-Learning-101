Метод ближайших соседей (k-nearest neighbours) основан на предположении о том, что схожие объекты всегда располагаются близко друг к другу. Здесь мы говорим о “расположении” признаков в пространстве — представьте себе координатные оси, на которых откладываются содержание спирта, кислотность, насыщенность цвета. Мы предполагаем, что классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему объекты обучающей выборки.

Близость объектов (или, иначе говоря, их сходство) определяется некоторой метрикой — отсюда и название типа алгоритма [метрический классификатор](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D1%80%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80).

К примеру, сорта винограда, используемые для изготовления вин **зинфандель** и **примитиво**, происходят от общего предка и по многим метрикам эти вина относятся к одному классу. Поэтому производители нередко указывают на упаковке, что данные два сорта — взаимозаменяемые.

<div class="hint">Предположение о схожести таких объектов называется <a href = "http://www.machinelearning.ru/wiki/index.php?title=%D0%93%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7%D0%B0_%D0%BA%D0%BE%D0%BC%D0%BF%D0%B0%D0%BA%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%B8">гипотезой компактности</a>. Она гласит, что схожие объекты гораздо чаще принадлежат одному классу, чем разным.</div>


\
Классификатор k-nearest neighbors не предполагает отдельной процедуры обучения — мы сразу переходим к классификации. Классификация каждого объекта состоит из следующих шагов:

- Рассчитать расстояния от классифицируемого объекта до каждого объекта в выборке;
- Отсортировать объекты по возрастанию расстояния $\rho$ до классифицируемого объекта (от наименьшего расстояния к наибольшему);
  $$
  \rho(u,x_1)\leq\rho(u,x_2)\leq...\leq\rho(u,x_l)$$
  где $x_i$ - $i$-й сосед объекта $u$.
- Выбрать первые $K$ объектов из отсортированного списка;
- Вернуть наиболее часто встречающуюся метку среди этих $K$ объектов. Здесь метка — это идентификатор класса (в нашем случае — идентификатор сорта вина).

$$
a(u, X^l) = \arg \max\limits_{y\in Y} \sum\limits_{y_i=y} w(i,u)
$$
где:
$y_i$ - класс $i$-го соседа $u$,

$w(i,u) = [i\leq k]$ – классы ближайших $i$ соседей $u$, 

$a(u,X^l)$ – наиболее распространенный среди них.


Мы предлагаем вам две функции для вычисления расстояния между объектами: [евклидова метрика](https://ru.wikipedia.org/wiki/%D0%95%D0%B2%D0%BA%D0%BB%D0%B8%D0%B4%D0%BE%D0%B2%D0%B0_%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%BA%D0%B0) (euclidean_dist) и [расстояние городских кварталов](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%B3%D0%BE%D1%80%D0%BE%D0%B4%D1%81%D0%BA%D0%B8%D1%85_%D0%BA%D0%B2%D0%B0%D1%80%D1%82%D0%B0%D0%BB%D0%BE%D0%B2) (taxicab_dist). В приведенном нами решении для определения сходства используется евклидово расстояние, но вы можете попробовать заменить его на расстояние городских кварталов и пронаблюдать за тем, как изменится результат классификации. Существуют и другие функции расстояния; выбор той или иной функции зависит от конкретной задачи. Они находятся в файле `distances.py`.

При выборе метрики необходимо стремиться к максимизации суммы расстояний между объектами разных классов и к минимизации расстояний между объектами внутри одного класса. Тогда отличающиеся классы будут лежать далеко друг от друга, а схожие — рядом.

Таким образом, например, крепкие вина с большой кислотностью будут отнесены к одному классу, а слабые с менее выраженным вкусом — к другому.
### Задание

Реализуйте функцию, предсказывающую метки класса по известным примерам. Функция `knn` должна принимать:
- обучающую выборку `X_train`, `y_train`;
- выборку, которую нужно классифицировать, `X_test`;
- количество соседей `k`;
- функцию расстояния `dist`.

Результатом функции является вектор `y_test`, хранящий классы, присвоенные каждому элементу `X_test` соответственно.

```python
def knn(X_train, y_train, X_test, k, dist):
    return #class for each x in the X_test
```

Заготовка для функции представлена в файле `metric_classification.py`.

В этом задании вам могут пригодиться следующие функции библиотеки **NumPy**:
- [numpy.argpartition](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argpartition.html) принимает некий массив и любой индекс k и возвращает новый массив, в котором на позиции k стоит элемент, занимающий позицию k в отсортированном исходном массиве; при этом меньшие значения располагаются слева от элемента k, а бóльшие значения — справа от элемента k. Данная функция поможет выявить ближайших соседей в массиве, отсортированном по расстоянию.
- [numpy.bincount](https://docs.scipy.org/doc/numpy/reference/generated/numpy.bincount.html) возвращает количество отличных элементов в массиве неотрицательных чисел. Данная функция поможет вычислить количество различных классов среди соседей.
- [numpy.argmax](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html) возвращает индексы элементов с наибольшими значениями. Эта функция позволит, к примеру, вычислить наиболее часто встречающиеся метки классов.
