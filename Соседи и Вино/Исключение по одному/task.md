### Выбор значения K

При `k = 1` алгоритм ближайшего соседа неустойчив к [шумовым выбросам](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9#.D0.9E.D1.82.D1.81.D0.B5.D0.B2_.D1.88.D1.83.D0.BC.D0.B0_.28.D0.B2.D1.8B.D0.B1.D1.80.D0.BE.D1.81.D0.BE.D0.B2.29): он дает ошибочные классификации не только
на объектах-выбросах, но и на ближайших к ним объектах других классов.

**Шумовой выброс** — экстремальное значение выборки, находящееся далеко за пределами других наблюдений. В данном контексте это объекты, ошибочно отнесенные к тому или иному классу. Причиной шумовых выбросов может быть сбой оборудования при замерах, человеческий фактор при обработке данных, уникальные характеристики некоторых вин и т. д.

При `k` равном размеру выборки, алгоритм, напротив, чрезмерно устойчив и вырождается в константу. В этом случае классификация будет определяться не схожестью признаков, а лишь тем, каких вин в выборке представлено больше. Так, в нашей выборке в файле **wine.csv** представлено 59 вин первого класса, 71 вино второго класса и 48 вин третьего. Если мы будем анализировать наиболее часто встречающийся класс среди 177 ближайших соседей, то для каждого из вин этим классом окажется 2.

Таким образом, крайние значения `k` нежелательны.

На практике оптимальное значения параметра `k` определяют по критерию скользящего контроля с **исключением объектов по одному** (leave one out):
${LOO(k, X^l) = \sum\limits_{i=1}^l [a(x_i; X^l \backslash \\{x_i\\}, k) \neq y_i] \rightarrow \min\limits_k}$.

Здесь каждый вариант выборки $X^l \backslash \\{x_i\\}$ используется в качестве обучающей, а само $\\{x_i\\}$ используется 
в качестве тестовой выборки. Если при обучении допущена ошибка и предсказанный класс не равен $\\{y_i\\}$, то увеличивается 
сумма ошибок для данного $k$.

$k$ с наименьшей суммарной ошибкой считается оптимальным, а мы выберем наименьшее из всех оптимальных $k$.

<div class="hint">
Вспомним формулу определения класса по $k$ соседям:
$$
\rho(u,x_1)\leq\rho(u,x_2)\leq...\leq\rho(u,x_l)$$
$x_i$ – i-й сосед объекта u

$y_i$ – класс i-го соседа u
$$
a(u, X^l) = \arg \max\limits_{y\in Y} \sum\limits_{y_i=y} w(i,u)
$$
$w(i,u) = [i\leq k]$ – классы ближайших i соседей u
$a(u, X^l)$ – наиболее распространенный среди них.
</div>

Исключив один объект из выборки и обучив алгоритм на всех остальных объектах, мы можем проверить работу алгоритма на примере исключенного объекта. Оптимальным `k` будет наименьшее значение, для которого при такой проверке количество верно определенных классов будет максимальным.

<div class="hint">
Если классифицируемый объект не исключать из обучающей выборки, то его ближайшим соседом всегда будет он сам и минимальное значение функции $LOO(k)$ будет достигаться при $k=1$. 
</div>

### Задание

Реализуйте функцию для выбора оптимального значения `k` с помощью [метода контроля по отдельным объектам](http://www.machinelearning.ru/wiki/index.php?title=%D0%A1%D0%BA%D0%BE%D0%BB%D1%8C%D0%B7%D1%8F%D1%89%D0%B8%D0%B9_%D0%BA%D0%BE%D0%BD%D1%82%D1%80%D0%BE%D0%BB%D1%8C#.D0.9A.D0.BE.D0.BD.D1.82.D1.80.D0.BE.D0.BB.D1.8C_.D0.BF.D0.BE_.D0.BE.D1.82.D0.B4.D0.B5.D0.BB.D1.8C.D0.BD.D1.8B.D0.BC_.D0.BE.D0.B1.D1.8A.D0.B5.D0.BA.D1.82.D0.B0.D0.BC_.28leave-one-out_CV.29)
(leave-one-out кросс-валидации). Функция должна принимать обучающую выборку и функцию расстояния:

      def loocv(X_train, y_train, dist):
          # ...
          return opt_k

Заготовка для функции находится в файле `crossvalidation.py`. 

Оцените точность и полноту предсказаний классификатора с оптимальным `k`и любыми двумя функциями расстояния.

Для этого импортируйте `loocv`, `precision_recall`, `euclidian_dist` и `taxicab_dist` в `task.py` и скомбинируйте все созданные нами функции в `main`:
```python
if __name__ == '__main__':
    wines = np.genfromtxt('wine.csv', delimiter=',')

    X, y = wines[:, 1:], np.array(wines[:, 0], dtype=np.int32)
    X_train, y_train, X_test, y_test = train_test_split(X, y, 0.6)
    y_euclidean_predicted = knn(X_train, y_train, X_test, 5, euclidean_dist)
    print_precision_recall(precision_recall(y_euclidean_predicted, y_test))

    euclidean_opt = loocv(X_train, y_train, euclidean_dist)
    taxicab_opt = loocv(X_train, y_train, taxicab_dist)

    print("optimal euclidian k = " + str(euclidean_opt))
    print("optimal taxicab k = " + str(taxicab_opt))

    y_euclidean_predicted = knn(X_train, y_train, X_test, euclidean_opt, euclidean_dist)
    print_precision_recall(precision_recall(y_euclidean_predicted, y_test))

    y_taxicab_predicted = knn(X_train, y_train, X_test, taxicab_opt, euclidean_dist)
    print_precision_recall(precision_recall(y_taxicab_predicted, y_test))
```
